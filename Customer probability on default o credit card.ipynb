{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe9574a",
   "metadata": {},
   "source": [
    "##        Artificial Neural Network for Classification\n",
    "Used to solve a data analytics challenge for a bank looking to determine consumer credit risk.\n",
    "\n",
    "This is an Artificial Neural Network that can predict, based on 24 attributes of\n",
    "a customer, if an individual customer will default on their payment next month for their credit card.\n",
    "\n",
    "In addition, we are be able to rank all the customers of the bank, based on \n",
    "their probability of default. To do that, we use the right Deep Learning model, \n",
    "one that is based on a probabilistic approach (need the output layer(dependent variable)\n",
    "to use the sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a70a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# Data Preprocessing\n",
    "# ===========================\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005ae463",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dataset Information:\n",
    "    Data Set Characteristics: Multivariate\n",
    "    Attribute Characteristics: Integer, Real\n",
    "    Number of Attributes: 24\n",
    "    Number of Instances: 30000\n",
    "    Source: \n",
    "        UCI Machine Learning Repository\n",
    "        institutions: (1) Department of Information Management, Chung Hua University, Taiwan. (2) Department of Civil Engineering, Tamkang University, Taiwan. \n",
    "    \n",
    "Dataset Attributes:\n",
    "    This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. \n",
    "    This study reviewed the literature and used the following 23 variables as explanatory variables: \n",
    "        \n",
    "    X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. \n",
    "    X2: Gender (1 = male; 2 = female). \n",
    "    X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n",
    "    X4: Marital status (1 = married; 2 = single; 3 = others). \n",
    "    X5: Age (year). \n",
    "    X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly(paid appropriately); 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n",
    "    X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005. \n",
    "    X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005.\n",
    "'''\n",
    "# Import the dataset\n",
    "# Note we have two headers in dataset, skip the extra row\n",
    "dataset = pd.read_excel('./Downloads/default-of-credit-card-clients.xls', skiprows=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84fdee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "\n",
    "# matrix of features (independent variables). \n",
    "# select: all rows. columnn indices 1 - 23 (exclude columns: id#)\n",
    "X = dataset.iloc[:, 1:24].values \n",
    "\n",
    "# matrix of dependent vars (output to predict)\n",
    "# select: all rows. last column\n",
    "y = dataset.iloc[:, -1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3808dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "# Encode categorical data (converting into numbers to make it comparable for \n",
    "# Regression) and create dummy vars if needed(avoid dummy var trap)\n",
    "\n",
    "# This step is not needed since our dataset has no categorical variables\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "# Split the dataset into the Training set and Test set\n",
    "# Train:  0.8 (80%) of obesrvations to train ANN.\n",
    "# Test: 0.2 (20%) of obesrvations to test ANN. \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d70a4c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "# Feature Scaling to standardize the range of independent variables or features of data (normalize).\n",
    "# we want to have standard normally distributed data and dont want to have one independent var dominating another one.\n",
    "# Also due to intensive calculations and parallel execution, scaling eases these calculations.\n",
    "\n",
    "# If one of the features has a broad range of values, the distance will be governed by\n",
    "# this particular feature. Therefore, the range of all features should be normalized so \n",
    "# that each feature contributes approximately proportionately to the final distance.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc95879",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''(you have to use the same two parameters μ and σ (values) that you used for centering the training set)\n",
    "sklearn's transform's fit() just calculates the parameters (e.g. μ and σ)\n",
    "and saves them as an internal objects state. \n",
    "Afterwards, you can call its transform() method to apply the transformation to a particular set of examples.\n",
    "'''\n",
    "X_train = sc.fit_transform(X_train) # Fit to data, then transform it.\n",
    "X_test = sc.transform(X_test) # Perform standardization by centering and scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b378b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                288       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 457\n",
      "Trainable params: 457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# In[6]:\n",
    "# Intitialize the ANN\n",
    "\n",
    "# ===========================\n",
    "# Make the ANN\n",
    "# ===========================\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential # to initialize ANN\n",
    "from keras.layers import Dense # to create layers in ANN\n",
    "\n",
    "# Initialising the ANN (we will define it as a sequence of layers or you can define a graph)\n",
    "classifier = Sequential()\n",
    "# Add the input layer AND the first hidden layer. (for initial layer specify the input nodes since we have no prev layer)\n",
    "classifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu', input_dim = 23))\n",
    "\n",
    "# Add the second hidden layer (use prev layer as input nodes)\n",
    "classifier.add(Dense(units = 12, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Add the output layer (NOTE: if 3 encoded categories for dependent variable need 3 nodes and softmax activator func)\n",
    "# choose sigmoid just like in logistic regression\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a085835b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Dense() function will randomly initialize the weights to small numbers close to 0 (but not 0)\\n2. place each feature in one input node e.g: 11 input vars => 11 input nodes\\n3. farward propagation pass inputs from left to right where the neuron will be activated in way\\n   where impact of each neurons activation is limited by the weights. (sum allweights, apply activation function)\\n   using rectifier func for input layer and sigmoid for outer layer(to get probabilities of customer leaving or staying) \\n4. compare predicted result to actual\\n5. back propagation from right to left. update the weights according to how much they are responsible for he error\\n   The learnign rate decides by how much we update the weights\\n6. repeat steps 1-5 and update weights after each observation (row) (reinforcement learning)\\n   Or: repeat steps 1-5 and updatee the weights only after a batch of observations (Batch learning)\\n7. When the whole training (all rows of data) passed through the ANN that makes an epoch. Redo more epochs\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. Dense() function will randomly initialize the weights to small numbers close to 0 (but not 0)\n",
    "2. place each feature in one input node e.g: 11 input vars => 11 input nodes\n",
    "3. farward propagation pass inputs from left to right where the neuron will be activated in way\n",
    "   where impact of each neurons activation is limited by the weights. (sum allweights, apply activation function)\n",
    "   using rectifier func for input layer and sigmoid for outer layer(to get probabilities of customer leaving or staying) \n",
    "4. compare predicted result to actual\n",
    "5. back propagation from right to left. update the weights according to how much they are responsible for he error\n",
    "   The learnign rate decides by how much we update the weights\n",
    "6. repeat steps 1-5 and update weights after each observation (row) (reinforcement learning)\n",
    "   Or: repeat steps 1-5 and updatee the weights only after a batch of observations (Batch learning)\n",
    "7. When the whole training (all rows of data) passed through the ANN that makes an epoch. Redo more epochs\n",
    "'''\n",
    "# use stochastic gradient decscent\n",
    "\n",
    "\n",
    "#Choose Number of nodes in hidden layer: as average number of nodes in hidden and output layer\n",
    "#Or experiment with parameter tuning, ross validation techniques\n",
    "# (23 + 1)/2 for hidden layer\n",
    "\n",
    "# initialize weights randomly close to 0: kernel_initializer = 'uniform'\n",
    "\n",
    "# using rectifier activation func for input layter and sigmoid for outer layer(to get probabilities) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b47891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "# Compile the ANN (apply Stochastic gradient descent for back propagation)\n",
    "# Stochastic gradient descent algorithm = adam. \n",
    "# loss function within adam alg, based on loss function that we need to optimize to find optimal weights \n",
    "\n",
    "# (for simple linear regression loss func is sum of squared errors. \n",
    "# in ML(perceptron NN using sigmoid activation function you obtain a logistic regression model):\n",
    "# looking into stochastic gradient descent loss func is NOT sum of squared errors but is a \n",
    "# logarithmic function called logarithmic loss )\n",
    "\n",
    "# so use binary logarithmic loss func b/c (binary_entropy = dependent var has binary outcome, if i categories = categorical_crossentropy)\n",
    "# criteria to evaluate our model metrics = ['accuracy'] (after weights updated, algo uses accuracy criterion to improve models performance) (when we fit accuracy will increase little by litle until reach top accuracy since we chose accuracy metric)\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1241d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4880 - accuracy: 0.7919 - val_loss: 0.4667 - val_accuracy: 0.8104\n",
      "Epoch 2/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4512 - accuracy: 0.8170 - val_loss: 0.4551 - val_accuracy: 0.8108\n",
      "Epoch 3/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4464 - accuracy: 0.8186 - val_loss: 0.4529 - val_accuracy: 0.8096\n",
      "Epoch 4/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4435 - accuracy: 0.8196 - val_loss: 0.4521 - val_accuracy: 0.8112\n",
      "Epoch 5/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4419 - accuracy: 0.8184 - val_loss: 0.4472 - val_accuracy: 0.8146\n",
      "Epoch 6/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4411 - accuracy: 0.8190 - val_loss: 0.4493 - val_accuracy: 0.8131\n",
      "Epoch 7/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4406 - accuracy: 0.8195 - val_loss: 0.4500 - val_accuracy: 0.8110\n",
      "Epoch 8/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4402 - accuracy: 0.8176 - val_loss: 0.4493 - val_accuracy: 0.8117\n",
      "Epoch 9/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4396 - accuracy: 0.8197 - val_loss: 0.4499 - val_accuracy: 0.8129\n",
      "Epoch 10/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4384 - accuracy: 0.8195 - val_loss: 0.4474 - val_accuracy: 0.8092\n",
      "Epoch 11/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4372 - accuracy: 0.8202 - val_loss: 0.4464 - val_accuracy: 0.8119\n",
      "Epoch 12/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4350 - accuracy: 0.8201 - val_loss: 0.4579 - val_accuracy: 0.8104\n",
      "Epoch 13/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4353 - accuracy: 0.8193 - val_loss: 0.4553 - val_accuracy: 0.8079\n",
      "Epoch 14/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4345 - accuracy: 0.8200 - val_loss: 0.4457 - val_accuracy: 0.8117\n",
      "Epoch 15/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4339 - accuracy: 0.8205 - val_loss: 0.4424 - val_accuracy: 0.8123\n",
      "Epoch 16/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4336 - accuracy: 0.8208 - val_loss: 0.4448 - val_accuracy: 0.8123\n",
      "Epoch 17/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4326 - accuracy: 0.8203 - val_loss: 0.4431 - val_accuracy: 0.8108\n",
      "Epoch 18/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4318 - accuracy: 0.8204 - val_loss: 0.4475 - val_accuracy: 0.8102\n",
      "Epoch 19/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4318 - accuracy: 0.8203 - val_loss: 0.4445 - val_accuracy: 0.8117\n",
      "Epoch 20/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4315 - accuracy: 0.8217 - val_loss: 0.4452 - val_accuracy: 0.8110\n",
      "Epoch 21/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4307 - accuracy: 0.8208 - val_loss: 0.4407 - val_accuracy: 0.8110\n",
      "Epoch 22/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4307 - accuracy: 0.8207 - val_loss: 0.4454 - val_accuracy: 0.8096\n",
      "Epoch 23/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4304 - accuracy: 0.8199 - val_loss: 0.4434 - val_accuracy: 0.8127\n",
      "Epoch 24/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4300 - accuracy: 0.8208 - val_loss: 0.4481 - val_accuracy: 0.8110\n",
      "Epoch 25/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4301 - accuracy: 0.8199 - val_loss: 0.4434 - val_accuracy: 0.8121\n",
      "Epoch 26/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4299 - accuracy: 0.8198 - val_loss: 0.4406 - val_accuracy: 0.8110\n",
      "Epoch 27/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4297 - accuracy: 0.8204 - val_loss: 0.4423 - val_accuracy: 0.8115\n",
      "Epoch 28/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4291 - accuracy: 0.8210 - val_loss: 0.4451 - val_accuracy: 0.8123\n",
      "Epoch 29/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4293 - accuracy: 0.8208 - val_loss: 0.4418 - val_accuracy: 0.8125\n",
      "Epoch 30/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4294 - accuracy: 0.8203 - val_loss: 0.4408 - val_accuracy: 0.8115\n",
      "Epoch 31/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4292 - accuracy: 0.8202 - val_loss: 0.4424 - val_accuracy: 0.8098\n",
      "Epoch 32/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4289 - accuracy: 0.8206 - val_loss: 0.4409 - val_accuracy: 0.8127\n",
      "Epoch 33/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4286 - accuracy: 0.8212 - val_loss: 0.4418 - val_accuracy: 0.8135\n",
      "Epoch 34/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4283 - accuracy: 0.8216 - val_loss: 0.4487 - val_accuracy: 0.8102\n",
      "Epoch 35/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4280 - accuracy: 0.8214 - val_loss: 0.4453 - val_accuracy: 0.8133\n",
      "Epoch 36/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4279 - accuracy: 0.8221 - val_loss: 0.4466 - val_accuracy: 0.8117\n",
      "Epoch 37/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4285 - accuracy: 0.8215 - val_loss: 0.4426 - val_accuracy: 0.8102\n",
      "Epoch 38/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4274 - accuracy: 0.8222 - val_loss: 0.4467 - val_accuracy: 0.8119\n",
      "Epoch 39/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4275 - accuracy: 0.8217 - val_loss: 0.4423 - val_accuracy: 0.8104\n",
      "Epoch 40/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4277 - accuracy: 0.8213 - val_loss: 0.4463 - val_accuracy: 0.8123\n",
      "Epoch 41/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4270 - accuracy: 0.8200 - val_loss: 0.4433 - val_accuracy: 0.8096\n",
      "Epoch 42/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4275 - accuracy: 0.8215 - val_loss: 0.4427 - val_accuracy: 0.8115\n",
      "Epoch 43/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4272 - accuracy: 0.8216 - val_loss: 0.4425 - val_accuracy: 0.8125\n",
      "Epoch 44/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4269 - accuracy: 0.8217 - val_loss: 0.4436 - val_accuracy: 0.8133\n",
      "Epoch 45/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4265 - accuracy: 0.8224 - val_loss: 0.4454 - val_accuracy: 0.8106\n",
      "Epoch 46/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4270 - accuracy: 0.8216 - val_loss: 0.4441 - val_accuracy: 0.8119\n",
      "Epoch 47/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4262 - accuracy: 0.8222 - val_loss: 0.4497 - val_accuracy: 0.8112\n",
      "Epoch 48/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4262 - accuracy: 0.8224 - val_loss: 0.4461 - val_accuracy: 0.8104\n",
      "Epoch 49/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4266 - accuracy: 0.8221 - val_loss: 0.4442 - val_accuracy: 0.8121\n",
      "Epoch 50/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4258 - accuracy: 0.8227 - val_loss: 0.4448 - val_accuracy: 0.8125\n",
      "Epoch 51/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4259 - accuracy: 0.8232 - val_loss: 0.4423 - val_accuracy: 0.8119\n",
      "Epoch 52/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4264 - accuracy: 0.8223 - val_loss: 0.4450 - val_accuracy: 0.8117\n",
      "Epoch 53/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4256 - accuracy: 0.8220 - val_loss: 0.4435 - val_accuracy: 0.8119\n",
      "Epoch 54/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4256 - accuracy: 0.8226 - val_loss: 0.4448 - val_accuracy: 0.8121\n",
      "Epoch 55/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4257 - accuracy: 0.8233 - val_loss: 0.4446 - val_accuracy: 0.8098\n",
      "Epoch 56/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4258 - accuracy: 0.8217 - val_loss: 0.4428 - val_accuracy: 0.8094\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4253 - accuracy: 0.8221 - val_loss: 0.4467 - val_accuracy: 0.8096\n",
      "Epoch 58/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4251 - accuracy: 0.8241 - val_loss: 0.4446 - val_accuracy: 0.8133\n",
      "Epoch 59/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4250 - accuracy: 0.8231 - val_loss: 0.4436 - val_accuracy: 0.8104\n",
      "Epoch 60/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4252 - accuracy: 0.8219 - val_loss: 0.4471 - val_accuracy: 0.8112\n",
      "Epoch 61/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4253 - accuracy: 0.8231 - val_loss: 0.4435 - val_accuracy: 0.8110\n",
      "Epoch 62/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4247 - accuracy: 0.8229 - val_loss: 0.4431 - val_accuracy: 0.8110\n",
      "Epoch 63/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4247 - accuracy: 0.8222 - val_loss: 0.4443 - val_accuracy: 0.8110\n",
      "Epoch 64/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4247 - accuracy: 0.8229 - val_loss: 0.4478 - val_accuracy: 0.8096\n",
      "Epoch 65/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4248 - accuracy: 0.8221 - val_loss: 0.4417 - val_accuracy: 0.8123\n",
      "Epoch 66/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4247 - accuracy: 0.8227 - val_loss: 0.4430 - val_accuracy: 0.8123\n",
      "Epoch 67/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4247 - accuracy: 0.8227 - val_loss: 0.4439 - val_accuracy: 0.8108\n",
      "Epoch 68/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4246 - accuracy: 0.8227 - val_loss: 0.4482 - val_accuracy: 0.8108\n",
      "Epoch 69/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4240 - accuracy: 0.8230 - val_loss: 0.4465 - val_accuracy: 0.8112\n",
      "Epoch 70/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4244 - accuracy: 0.8221 - val_loss: 0.4439 - val_accuracy: 0.8087\n",
      "Epoch 71/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4241 - accuracy: 0.8229 - val_loss: 0.4426 - val_accuracy: 0.8102\n",
      "Epoch 72/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4246 - accuracy: 0.8232 - val_loss: 0.4462 - val_accuracy: 0.8100\n",
      "Epoch 73/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4245 - accuracy: 0.8241 - val_loss: 0.4441 - val_accuracy: 0.8108\n",
      "Epoch 74/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4240 - accuracy: 0.8237 - val_loss: 0.4435 - val_accuracy: 0.8110\n",
      "Epoch 75/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4241 - accuracy: 0.8232 - val_loss: 0.4443 - val_accuracy: 0.8110\n",
      "Epoch 76/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4240 - accuracy: 0.8228 - val_loss: 0.4435 - val_accuracy: 0.8092\n",
      "Epoch 77/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4239 - accuracy: 0.8222 - val_loss: 0.4424 - val_accuracy: 0.8119\n",
      "Epoch 78/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4239 - accuracy: 0.8230 - val_loss: 0.4413 - val_accuracy: 0.8110\n",
      "Epoch 79/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4236 - accuracy: 0.8225 - val_loss: 0.4421 - val_accuracy: 0.8112\n",
      "Epoch 80/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4237 - accuracy: 0.8234 - val_loss: 0.4410 - val_accuracy: 0.8100\n",
      "Epoch 81/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4239 - accuracy: 0.8226 - val_loss: 0.4421 - val_accuracy: 0.8098\n",
      "Epoch 82/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4239 - accuracy: 0.8223 - val_loss: 0.4464 - val_accuracy: 0.8115\n",
      "Epoch 83/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4243 - accuracy: 0.8225 - val_loss: 0.4412 - val_accuracy: 0.8119\n",
      "Epoch 84/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4237 - accuracy: 0.8236 - val_loss: 0.4436 - val_accuracy: 0.8096\n",
      "Epoch 85/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4232 - accuracy: 0.8235 - val_loss: 0.4425 - val_accuracy: 0.8108\n",
      "Epoch 86/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4237 - accuracy: 0.8236 - val_loss: 0.4489 - val_accuracy: 0.8108\n",
      "Epoch 87/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4234 - accuracy: 0.8234 - val_loss: 0.4422 - val_accuracy: 0.8119\n",
      "Epoch 88/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4231 - accuracy: 0.8227 - val_loss: 0.4427 - val_accuracy: 0.8112\n",
      "Epoch 89/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4236 - accuracy: 0.8230 - val_loss: 0.4426 - val_accuracy: 0.8110\n",
      "Epoch 90/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4234 - accuracy: 0.8241 - val_loss: 0.4425 - val_accuracy: 0.8123\n",
      "Epoch 91/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4231 - accuracy: 0.8231 - val_loss: 0.4434 - val_accuracy: 0.8115\n",
      "Epoch 92/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4236 - accuracy: 0.8231 - val_loss: 0.4434 - val_accuracy: 0.8098\n",
      "Epoch 93/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4232 - accuracy: 0.8228 - val_loss: 0.4422 - val_accuracy: 0.8119\n",
      "Epoch 94/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4228 - accuracy: 0.8229 - val_loss: 0.4418 - val_accuracy: 0.8117\n",
      "Epoch 95/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4233 - accuracy: 0.8232 - val_loss: 0.4435 - val_accuracy: 0.8119\n",
      "Epoch 96/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4230 - accuracy: 0.8233 - val_loss: 0.4415 - val_accuracy: 0.8125\n",
      "Epoch 97/100\n",
      "1920/1920 [==============================] - 4s 2ms/step - loss: 0.4232 - accuracy: 0.8227 - val_loss: 0.4428 - val_accuracy: 0.8096\n",
      "Epoch 98/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4226 - accuracy: 0.8231 - val_loss: 0.4448 - val_accuracy: 0.8123\n",
      "Epoch 99/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4229 - accuracy: 0.8231 - val_loss: 0.4434 - val_accuracy: 0.8092\n",
      "Epoch 100/100\n",
      "1920/1920 [==============================] - 3s 2ms/step - loss: 0.4230 - accuracy: 0.8227 - val_loss: 0.4445 - val_accuracy: 0.8100\n"
     ]
    }
   ],
   "source": [
    "# In[8]:\n",
    "# Fit the ANN to the Training set (experimment with batch size and epochs)\n",
    "# batch_size = 10, epochs = 100\n",
    "# loss: 0.4234 - acc: 0.8193\n",
    "hist=classifier.fit(X_train, y_train, batch_size = 10, epochs = 100,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f363674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8227083086967468, 0.8100000023841858)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_accu=hist.history['val_accuracy'][-1]\n",
    "accu=hist.history['accuracy'][-1]\n",
    "accu,val_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "085bcd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.8243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4355347752571106, 0.8243333101272583]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e71adef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66558254],\n",
       "       [0.29539126],\n",
       "       [0.22784236],\n",
       "       ...,\n",
       "       [0.12836903],\n",
       "       [0.73417   ],\n",
       "       [0.2619006 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict=classifier.predict(X_test)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b123598",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[predict>0.5]=1\n",
    "predict[predict<0.5]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e81d816c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ca4b71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8243333333333334"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test=y_test.reshape(6000,1)\n",
    "np.sum(y_test==predict)/predict.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d38d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4d1ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the Confusion Matrix to evaluate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bfdc0627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4451,  252],\n",
       "       [ 802,  495]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
